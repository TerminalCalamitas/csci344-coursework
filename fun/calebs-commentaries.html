<!DOCTYPE>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Caleb's Commentaries</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image" href="../favicon.png">
  </head>
    <header>
      <h1>Caleb's Commentaries</h1>
      <p>This is a page for me to ramble about various topics.</p>
    </header>
  <body>
    <section class="sidebar">
      <h2>Table of Contents</h2>
      <ul>
        <li><b><a href="../index.html">Back Home</a></b></li>
        <li><a href="#topic1">How do LLM's work?</a></li>
        <li><a href="#topic2">Applications for AI in Education</a></li>
        <li><a href="#topic3">How does AI copyright work?</a></li>
        <li><a href="#topic4">Fine Tuning a LLM</a></li>
      </ul>
    </section>

    <section class="topic" id="topic1">
      <h2><b>How do LLM's work?</b></h2>
      <p>
        LLM's are very complicated math models that use given data to make predictions about what is most likely to come next, <b>they are not intelligent and cannot understand or make decisions</b>.
        <br>Currently companies have found that by giving more data to the LLMs during the training process, the LLMs are able to make more accurate predictions since they have more information about how people usually talk.
        When the LLM is given an input prompt, it turns that prompt into a series of numbers called tokens that represent the text. 
        The LLM then uses a series of matrix multiplications operations on the input tokens to determine the token that is most likely to come next. 
        Unlike what is commonly believed, none of the LLMs are able to understand the input or the data it was trained on. 
        While it is possible to use LLMs to generate the materials that the LLM was trained on, <b>the model is never able to direcly copy sections from the input data</b>.
      </p>
    </section>

    <section class="topic" id="topic2">
      <h2><b>Applications for AI in Education</b></h2>
      <p>&emsp; I believe that AI should be viewed as an academic tool rather than something harmful that needs
to be avoided. While I wasn’t alive when the web started becoming popular I believe that schools and
educators would have had some of the same concerns about the internet as people have about it today.
The internet, just like AI has the possibillity to hinder learning if the student just uses it for plaigerism
rather than engaging with the matierial they are using. Just as the internet has become a core part of
modern education, I belive that if students are instructed in the proper way to use AI, <b>it has the potential
to be just as helpful if not more helpful than the internet for education.</b><br><br>
&emsp; It is very simple to get an LLM to respond in a certain manner and so while information may be
presented by teachers or websites in a specific way, if someone is having trouble understanding, they
can ask AI to explain the material in whatever manner is <b>most helpful to them.</b> This point does bring up
the problem of students using AI to get out of learning by just copying the output from LLM models.
This is another situation where there are many similarities to when the internet was new. If there are not
expectations and appropriate limitations put in place then students will take the easiest way to get
something done.<br><br>
&emsp; I have used AI in some of my classes before and I believe it helped me better understand what I
was doing and what I should be doing. For example, when running C programs I often got the error
`Segmentation Fault` which doesn’t give much helpful information about what I did wrong. By asking
an LLM questions during my debugging process I was better able to understand what the problem was
as well as being able to fix the problem faster. <b>As a conclusion, I believe that while AI can be used
negatively in a student’s education, universities should adapt their teaching to encourage a healthy
relationship between students and AI.</b>
      </p>
    </section>

    <section class="topic" id="topic3">
      <h2><b>How does AI copyright work?</b></h2>
      <p>
        I would like to start by saying that a lot of what I say is based on the great video by <a href="https://youtu.be/pt7GtDMTd3k?si=z-0_zcZToSmogztM">DougDoug</a> so I encourage you to watch his video as I think it goes into much better detail.<br>
        <br>AI is in a weird legal situation because it's so new that there have not been many cases around it so there isn't a lot of legal precedent. One place where this is very important is copyright law. Typically whenever someone creates something, whether a piece of writing, a video, song, or any other form of creative work <b>they own the copyright of their creation.</b>
        While this is a bit of a simplification, having copyright oversomething means that you own the thing created and you can decide how it is used by others.
        <br>When talking about AI generated content, things get a bit more confusing. There are 3 potential entities who would own the copyright to the created content, the AI company, the AI model itself, or the end user. Companies like OpenAI and Deepseek state in their Terms of Service that the user gets the rights to anything created from the user's input. This sounds like the user simply owns the copyright of the content, but there are some things that make things a bit more complicated.
        <br>To determine if the user can have the copyright we have to determine that OpenAI and Deepseek actually have the rights to the content created by the AI in order for them to give it away. In the case of <i>Naruto v. David Slater et al</i> it was determined that even though David Slater set everything up for a monkey to take a picture of itself, because the monkey took the picture, the monkey owned the copyright rather than David Slater. This means that even though you set everything up for the AI to create something, you do not own the copyright, the AI does. In the same case it was determined that since the owner of the copyright was a monkey and not human, it actually couldn't own the copyright. This means that the copyright of the image is in a weird place where <b>no one actually owns the copyright of the image</b>.
        <br>This is a very important case when it comes to AI because it means that since the AI did the work to create the content the model owns the rights to the content, however, since the model isn't a human, it can't own the copyright. This means that even though AI companies are giving you "the rights" to the produced content, they aren't giving you anything since those rights <b>don't actually exist.</b>
        <br>While it's not really a problem in most scenarios, if you use AI in the writing of code for your job, that code is in a weird legal place where you don't own the code so other people can take the code and use it without any repercussions since it isn't copywritten.
      </p>
    </section>

    <section class="topic" id="topic4">
      <h2><b>Fine Tuning a LLM</b></h2>
      <p>
        When using an LLM for a specific purpose it can be more desireable to have the LLM respond in a given style without having to spend a lot of time developing a system prompt that gives the desired response style. 
        One way to get such a specific response style is to fine tune the LLM on a dataset that has the desired response style.
        <br>To fine tune an LLM you need a few things, 1. A dataset that has many examples of the desired response style, 2. A base LLM that you can fine tune, and 3. A computer powerful enough to run the fine tuning process.
        I decided to fine tune an LLM off of my own messages to see if I could get the LLM to respond in a similar manner to how I do. For the dataset I used 25 various examples of conversations I had with people. For the base LLM I used Deepseek v3 1.7B distill since it was the only LLM that my computer could fine tune consistently. Finally, for my computer I used my laptop which has an intel 15-1340P cpu, 32GB of ram, and an external Nvidia 3050 GPU with 8GB of VRAM.
        <br>To fine tune (I used LLaMA Factory for all the training steps) the model, the first step is to train a LoRA which is kind of like a mask that you can apply to a model to make it respond in a certain manner. To train the LoRA I formatted all of my example conversations into the sharegpt file format then ran the training process. After the LoRA was trained I applied it to the base model and then used the LLaMA Factory export feature to make a new .safetensors file that I could then load to talk to my fine tuned model. If I wanted to use the model in LM studio I would have to convert the .safetensors file into a .GGUF file but since I wasn't thrilled with the results I didn't do that.
        <br>I ran the fine tuning process for 1000 steps which took about an hour to process. While the fine tuned model works to generate answers, the limited size of the model (being only 1.7B parameters) means that the model is actually really dumb and doesn't give good responses very often.
        <br>While my model is really dumb I'm hoping to get a more powerful GPU so that I can fine-tune a better model as the process for fine tuneing was really fun. In addition to a better GPU I also plan to look through more of my sent messages to find more examples of my conversational style since more data tends to make better LLMs.
      </p>
    </section>

  </body>
</html>

